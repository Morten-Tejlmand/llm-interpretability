{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8664f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#code block #1 — CONFIG\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "# ----------------- PATHS: EDIT THESE -----------------\n",
    "BASE_PRISM_DIR = r\"C:\\Users\\thors\\Documents\\GitHub\\prism\"\n",
    "\n",
    "# PRISM descriptions for GPT-2 small SAE (same as before)\n",
    "DESCRIPTION_DIR = os.path.join(\n",
    "    BASE_PRISM_DIR,\n",
    "    r\"descriptions\\gemini-1-5-pro\\gpt2-small-sae\"\n",
    ")\n",
    "\n",
    "# PRISM metrics CSV (polysemanticity + quality)\n",
    "METRICS_CSV = os.path.join(\n",
    "    BASE_PRISM_DIR,\n",
    "    r\"results\\meta-evaluation_cosine-similarity_target-gpt2-small-sae_textgen-gemini-1-5-pro_mean_evalgen-gemini-1-5-pro_cosmopedia_1000.csv\"\n",
    ")\n",
    "\n",
    "# Your prompt CSV (generated externally)\n",
    "PROMPTS_CSV = r\"C:\\Users\\thors\\Documents\\GitHub\\llm-interpretability\\gpt-2_layer0_prompts.csv\"   # must include: unit,text,sample_type (and ideally layer)\n",
    "\n",
    "# Output dir\n",
    "OUTPUT_DIR = os.path.join(BASE_PRISM_DIR, \"runtime_collision_results\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------- MODELS -----------------\n",
    "EVAL_MODEL_NAME = \"gpt2-small\"\n",
    "SAE_RELEASE = \"gpt2-small-resid-post-v5-32k\"\n",
    "\n",
    "# ----------------- EXPERIMENT -----------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# If your CSV doesn’t have a layer column, set it here (you said layer 0)\n",
    "DEFAULT_LAYER_IF_MISSING = 0\n",
    "\n",
    "# Optional: save raw results (can be big)\n",
    "SAVE_RAW_RESULTS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac62170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PRISM layer 0 descriptions: 95\n",
      "Merged rows: 95\n"
     ]
    }
   ],
   "source": [
    "#code block #2 — LOAD PRISM (LAYER 0 ONLY) + MERGE METRICS\n",
    "# Load metrics once\n",
    "metrics_df = pd.read_csv(METRICS_CSV)\n",
    "\n",
    "# Load descriptions (restrict to layer 0 to keep it light)\n",
    "desc_files_layer0 = glob.glob(os.path.join(DESCRIPTION_DIR, \"gpt2-small-sae_layer-0_unit-*.csv\"))\n",
    "if len(desc_files_layer0) == 0:\n",
    "    raise FileNotFoundError(\"No layer-0 description CSVs found. Check DESCRIPTION_DIR.\")\n",
    "\n",
    "desc_dfs = [pd.read_csv(p) for p in desc_files_layer0]\n",
    "descriptions_df = pd.concat(desc_dfs, ignore_index=True)\n",
    "\n",
    "# Merge\n",
    "merged_df = descriptions_df.merge(metrics_df, on=[\"layer\", \"unit\"], how=\"left\")\n",
    "\n",
    "# For joining later\n",
    "metrics_cols = [\"layer\", \"unit\", \"cosine_similarity\", \"cosine_similarity_random\", \"max_auc\", \"max_mad\"]\n",
    "metrics_unique = merged_df[metrics_cols].drop_duplicates(subset=[\"layer\", \"unit\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded PRISM layer 0 descriptions:\", len(descriptions_df))\n",
    "print(\"Merged rows:\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f16296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded eval model: gpt2-small\n"
     ]
    }
   ],
   "source": [
    "#code block #3 — LOAD EVAL MODEL + SAE LOADER\n",
    "eval_model = HookedTransformer.from_pretrained(\n",
    "    EVAL_MODEL_NAME,\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "eval_model.eval()\n",
    "print(\"Loaded eval model:\", EVAL_MODEL_NAME)\n",
    "\n",
    "# Max PRISM unit per layer (sanity check)\n",
    "max_unit_by_layer = merged_df.groupby(\"layer\")[\"unit\"].max().to_dict()\n",
    "\n",
    "sae_cache = {}\n",
    "\n",
    "def hook_name_for_layer(layer: int) -> str:\n",
    "    return f\"blocks.{layer}.hook_resid_post\"\n",
    "\n",
    "def get_sae_for_layer(layer: int) -> SAE:\n",
    "    if layer in sae_cache:\n",
    "        return sae_cache[layer]\n",
    "\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_post\"\n",
    "    sae, cfg, sparsity = SAE.from_pretrained_with_cfg_and_sparsity(\n",
    "        SAE_RELEASE,\n",
    "        sae_id,\n",
    "        device=device,\n",
    "    )\n",
    "    sae.eval()\n",
    "\n",
    "    # simple check\n",
    "    if layer in max_unit_by_layer:\n",
    "        if max_unit_by_layer[layer] >= sae.cfg.d_sae:\n",
    "            raise ValueError(\n",
    "                f\"Layer {layer}: PRISM max unit={max_unit_by_layer[layer]} but SAE d_sae={sae.cfg.d_sae}. \"\n",
    "                \"Mismatch between PRISM and SAE release.\"\n",
    "            )\n",
    "\n",
    "    sae_cache[layer] = sae\n",
    "    return sae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5284c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded prompts: 114\n",
      "sample_type\n",
      "single    95\n",
      "mixed     19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#code block #4 — LOAD YOUR PROMPTS CSV + NORMALIZE\n",
    "texts_df = pd.read_csv(PROMPTS_CSV)\n",
    "\n",
    "# Normalize column names\n",
    "if \"single/mixed\" in texts_df.columns:\n",
    "    texts_df = texts_df.rename(columns={\"single/mixed\": \"sample_type\"})\n",
    "\n",
    "required = {\"unit\", \"text\", \"sample_type\"}\n",
    "missing = required - set(texts_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}. Found columns: {list(texts_df.columns)}\")\n",
    "\n",
    "# Add layer if missing\n",
    "if \"layer\" not in texts_df.columns:\n",
    "    texts_df[\"layer\"] = DEFAULT_LAYER_IF_MISSING\n",
    "\n",
    "# Make sample_type robust\n",
    "texts_df[\"sample_type\"] = (\n",
    "    texts_df[\"sample_type\"]\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .replace({\"0\": \"single\", \"1\": \"mixed\", \"false\": \"single\", \"true\": \"mixed\"})\n",
    ")\n",
    "\n",
    "texts_df = texts_df[texts_df[\"sample_type\"].isin([\"single\", \"mixed\"])].reset_index(drop=True)\n",
    "\n",
    "# Basic cleanup\n",
    "texts_df[\"text\"] = texts_df[\"text\"].astype(str)\n",
    "\n",
    "# Sanity summary\n",
    "print(\"Loaded prompts:\", len(texts_df))\n",
    "print(texts_df[\"sample_type\"].value_counts(dropna=False))\n",
    "\n",
    "# Ensure we can compute collision penalty (need both single and mixed per unit)\n",
    "counts = texts_df.groupby([\"layer\", \"unit\"])[\"sample_type\"].nunique()\n",
    "bad = counts[counts < 2]\n",
    "if len(bad):\n",
    "    print(\"Warning: some (layer, unit) missing single or mixed; collision penalty will be NaN for them.\")\n",
    "    print(bad.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f21c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code block #5 — LOSS FUNCTIONS (BASELINE + SAE ABLATION)\n",
    "from functools import partial\n",
    "\n",
    "def _per_sample_loss_from_logits(logits: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: [B, S, V]\n",
    "    tokens: [B, S]\n",
    "    returns: [B] mean NLL per token (next-token)\n",
    "    \"\"\"\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = tokens[:, 1:]\n",
    "\n",
    "    logp = torch.log_softmax(shift_logits, dim=-1)\n",
    "    nll = -logp.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)  # [B, S-1]\n",
    "\n",
    "    # GPT-2 usually no padding here; token_count = S-1\n",
    "    token_counts = torch.full((tokens.shape[0],), fill_value=shift_labels.shape[1], device=tokens.device, dtype=torch.float32)\n",
    "    return nll.sum(dim=1) / token_counts\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_losses_baseline(texts: list[str]) -> np.ndarray:\n",
    "    if not texts:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    tokens = eval_model.to_tokens(texts).to(device)\n",
    "    logits = eval_model(tokens, return_type=\"logits\")\n",
    "    losses = _per_sample_loss_from_logits(logits, tokens)\n",
    "    return losses.cpu().numpy()\n",
    "\n",
    "def sae_intervention_hook(\n",
    "    acts: torch.Tensor,\n",
    "    hook,\n",
    "    sae: SAE,\n",
    "    feature_idx: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ablate one SAE feature at resid_post by setting it to 0.\n",
    "    acts: [B, S, d_model]\n",
    "    \"\"\"\n",
    "    B, S, D = acts.shape\n",
    "    flat = acts.reshape(-1, D)\n",
    "    feats = sae.encode(flat)           # [B*S, d_sae]\n",
    "    feats[:, feature_idx] = 0.0\n",
    "    recon = sae.decode(feats)          # [B*S, d_model]\n",
    "    return recon.reshape(B, S, D)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_losses_ablate_feature(texts: list[str], layer: int, unit: int) -> np.ndarray:\n",
    "    if not texts:\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "    sae = get_sae_for_layer(layer)\n",
    "    hook_name = hook_name_for_layer(layer)\n",
    "\n",
    "    tokens = eval_model.to_tokens(texts).to(device)\n",
    "\n",
    "    hook_fn = partial(sae_intervention_hook, sae=sae, feature_idx=int(unit))\n",
    "    logits = eval_model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"logits\",\n",
    "        fwd_hooks=[(hook_name, hook_fn)],\n",
    "    )\n",
    "\n",
    "    losses = _per_sample_loss_from_logits(logits, tokens)\n",
    "    return losses.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b23bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature-level collision metrics for 19 features\n",
      "   layer   unit  collision_penalty  collision_penalty_rel  delta_single_mean  \\\n",
      "0      0    259           0.535411               0.113635          -0.546685   \n",
      "1      0   2002           0.112652               0.026823          -0.070695   \n",
      "2      0   2236           1.246048               0.227148          -1.244619   \n",
      "3      0   2332           0.207311               0.047592          -0.224504   \n",
      "4      0   3358           0.460175               0.095806          -0.416933   \n",
      "5      0   5369           0.662599               0.132548          -0.647283   \n",
      "6      0   8966           0.989261               0.180271          -0.988548   \n",
      "7      0   9661           0.894785               0.166979          -0.856334   \n",
      "8      0  10233           0.234287               0.052397          -0.212474   \n",
      "9      0  10917           0.362007               0.080967          -0.359046   \n",
      "\n",
      "   delta_mixed_mean  base_single_mean  base_mixed_mean  baseline_gap  \\\n",
      "0         -0.011274          4.703061         4.327895     -0.375165   \n",
      "1          0.041956          4.080109         4.418283      0.338173   \n",
      "2          0.001429          5.487831         4.064265     -1.423566   \n",
      "3         -0.017194          4.311644         3.840111     -0.471533   \n",
      "4          0.043242          4.867513         4.260201     -0.607312   \n",
      "5          0.015315          5.022802         4.162352     -0.860450   \n",
      "6          0.000714          5.488499         4.497635     -0.990864   \n",
      "7          0.038451          5.420510         4.272917     -1.147593   \n",
      "8          0.021813          4.510216         4.125352     -0.384863   \n",
      "9          0.002961          4.470951         4.484475      0.013525   \n",
      "\n",
      "   cosine_similarity  max_auc   max_mad  \n",
      "0           0.467154  0.50000  0.000000  \n",
      "1           0.289363  0.54305  0.253615  \n",
      "2           0.269437  0.53035  0.277960  \n",
      "3           0.447954  0.50000  0.000000  \n",
      "4           0.360371  0.50000  0.000000  \n",
      "5           0.654788  0.50000  0.000000  \n",
      "6           0.230130  0.50000  0.000000  \n",
      "7           0.400420  0.50000  0.000000  \n",
      "8           0.568858  0.50000  0.000000  \n",
      "9           0.541213  0.50000  0.000000  \n",
      "Saved feature-level: C:\\Users\\thors\\Documents\\GitHub\\prism\\runtime_collision_results\\runtime_collision_from_csv_feature_level.csv\n"
     ]
    }
   ],
   "source": [
    "#code block #6 — RUN COLLISION EXPERIMENT ON YOUR CSV (ABLATION PENALTY + BASELINE LOSS DIAGNOSTICS)\n",
    "\n",
    "results_rows = []\n",
    "\n",
    "for (layer, unit), g in texts_df.groupby([\"layer\", \"unit\"]):\n",
    "    layer = int(layer)\n",
    "    unit = int(unit)\n",
    "\n",
    "    singles = g[g[\"sample_type\"] == \"single\"][\"text\"].tolist()\n",
    "    mixed   = g[g[\"sample_type\"] == \"mixed\"][\"text\"].tolist()\n",
    "\n",
    "    # Baseline + ablate on singles\n",
    "    base_s = compute_losses_baseline(singles)\n",
    "    ablt_s = compute_losses_ablate_feature(singles, layer=layer, unit=unit)\n",
    "\n",
    "    # Baseline + ablate on mixed\n",
    "    base_m = compute_losses_baseline(mixed)\n",
    "    ablt_m = compute_losses_ablate_feature(mixed, layer=layer, unit=unit)\n",
    "\n",
    "    # Store per-sample results (minimal)\n",
    "    for i, (b, a) in enumerate(zip(base_s, ablt_s)):\n",
    "        results_rows.append({\n",
    "            \"layer\": layer,\n",
    "            \"unit\": unit,\n",
    "            \"sample_type\": \"single\",\n",
    "            \"sample_idx\": i,\n",
    "            \"baseline\": float(b),\n",
    "            \"ablate\": float(a),\n",
    "        })\n",
    "    for i, (b, a) in enumerate(zip(base_m, ablt_m)):\n",
    "        results_rows.append({\n",
    "            \"layer\": layer,\n",
    "            \"unit\": unit,\n",
    "            \"sample_type\": \"mixed\",\n",
    "            \"sample_idx\": i,\n",
    "            \"baseline\": float(b),\n",
    "            \"ablate\": float(a),\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "results_df[\"delta\"] = results_df[\"ablate\"] - results_df[\"baseline\"]\n",
    "\n",
    "# -------- Feature-level summary with baseline-loss diagnostics --------\n",
    "\n",
    "feat_summary = results_df.groupby([\"layer\", \"unit\", \"sample_type\"]).agg(\n",
    "    base_loss_mean=(\"baseline\", \"mean\"),\n",
    "    base_loss_std=(\"baseline\", \"std\"),\n",
    "    delta_mean=(\"delta\", \"mean\"),\n",
    "    delta_std=(\"delta\", \"std\"),\n",
    "    n=(\"delta\", \"size\"),\n",
    ").reset_index()\n",
    "\n",
    "single_feat = feat_summary[feat_summary[\"sample_type\"] == \"single\"].rename(columns={\n",
    "    \"base_loss_mean\": \"base_single_mean\",\n",
    "    \"base_loss_std\": \"base_single_std\",\n",
    "    \"delta_mean\": \"delta_single_mean\",\n",
    "    \"delta_std\": \"delta_single_std\",\n",
    "    \"n\": \"n_single\",\n",
    "}).drop(columns=[\"sample_type\"])\n",
    "\n",
    "mixed_feat = feat_summary[feat_summary[\"sample_type\"] == \"mixed\"].rename(columns={\n",
    "    \"base_loss_mean\": \"base_mixed_mean\",\n",
    "    \"base_loss_std\": \"base_mixed_std\",\n",
    "    \"delta_mean\": \"delta_mixed_mean\",\n",
    "    \"delta_std\": \"delta_mixed_std\",\n",
    "    \"n\": \"n_mixed\",\n",
    "}).drop(columns=[\"sample_type\"])\n",
    "\n",
    "feat = single_feat.merge(mixed_feat, on=[\"layer\", \"unit\"], how=\"outer\")\n",
    "\n",
    "# Collision penalty (absolute)\n",
    "feat[\"collision_penalty\"] = feat[\"delta_mixed_mean\"] - feat[\"delta_single_mean\"]\n",
    "\n",
    "# Baseline gap (mixed harder than single?)\n",
    "feat[\"baseline_gap\"] = feat[\"base_mixed_mean\"] - feat[\"base_single_mean\"]\n",
    "\n",
    "# Relative deltas (robustness diagnostics)\n",
    "feat[\"delta_single_rel\"] = feat[\"delta_single_mean\"] / feat[\"base_single_mean\"]\n",
    "feat[\"delta_mixed_rel\"]  = feat[\"delta_mixed_mean\"]  / feat[\"base_mixed_mean\"]\n",
    "feat[\"collision_penalty_rel\"] = feat[\"delta_mixed_rel\"] - feat[\"delta_single_rel\"]\n",
    "\n",
    "# Join PRISM metrics (layer 0 present; others will be NaN)\n",
    "feat_with_metrics = feat.merge(metrics_unique, on=[\"layer\", \"unit\"], how=\"left\")\n",
    "\n",
    "print(\"Computed feature-level collision metrics for\", len(feat_with_metrics), \"features\")\n",
    "print(feat_with_metrics[\n",
    "    [\"layer\",\"unit\",\n",
    "     \"collision_penalty\",\"collision_penalty_rel\",\n",
    "     \"delta_single_mean\",\"delta_mixed_mean\",\n",
    "     \"base_single_mean\",\"base_mixed_mean\",\"baseline_gap\",\n",
    "     \"cosine_similarity\",\"max_auc\",\"max_mad\"]\n",
    "].head(10))\n",
    "\n",
    "# Optional saves\n",
    "if SAVE_RAW_RESULTS:\n",
    "    raw_path = os.path.join(OUTPUT_DIR, \"runtime_collision_from_csv_raw.csv\")\n",
    "    results_df.to_csv(raw_path, index=False)\n",
    "    print(\"Saved raw:\", raw_path)\n",
    "\n",
    "feat_path = os.path.join(OUTPUT_DIR, \"runtime_collision_from_csv_feature_level.csv\")\n",
    "feat_with_metrics.to_csv(feat_path, index=False)\n",
    "print(\"Saved feature-level:\", feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "483608f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QC keep rate overall: 0.0\n",
      "QC keep rate by sample_type:\n",
      "sample_type\n",
      "mixed     0.0\n",
      "single    0.0\n",
      "Name: keep, dtype: float64\n",
      "Saved QC table: C:\\Users\\thors\\Documents\\GitHub\\prism\\runtime_collision_results\\prompt_qc_table.csv\n"
     ]
    }
   ],
   "source": [
    "#code block #7 — OPTIONAL QC BLOCK (PROMPT QUALITY + FEATURE ACTIVATION)\n",
    "# Toggle this on/off\n",
    "RUN_QC = True\n",
    "\n",
    "if RUN_QC:\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    def simple_tokenize(s: str):\n",
    "        return re.findall(r\"\\w+|[^\\w\\s]\", str(s).lower())\n",
    "\n",
    "    def repetition_metrics(text: str, ngram_n: int = 3):\n",
    "        toks = simple_tokenize(text)\n",
    "        n = len(toks)\n",
    "        if n == 0:\n",
    "            return {\"n_tokens\": 0, \"unique_token_ratio\": 0.0, \"max_ngram_count\": 0}\n",
    "\n",
    "        unique_ratio = len(set(toks)) / n\n",
    "        if n < ngram_n:\n",
    "            max_ng = 0\n",
    "        else:\n",
    "            ngrams = [tuple(toks[i:i+ngram_n]) for i in range(n-ngram_n+1)]\n",
    "            counts = Counter(ngrams)\n",
    "            max_ng = max(counts.values()) if counts else 0\n",
    "\n",
    "        return {\"n_tokens\": n, \"unique_token_ratio\": float(unique_ratio), \"max_ngram_count\": int(max_ng)}\n",
    "\n",
    "    def get_descriptions_for_feature(layer: int, unit: int) -> list[str]:\n",
    "        g = merged_df[(merged_df[\"layer\"] == layer) & (merged_df[\"unit\"] == unit)]\n",
    "        return g[\"description\"].dropna().astype(str).tolist()\n",
    "\n",
    "    def tfidf_coverage_scores(text: str, descriptions: list[str]):\n",
    "        if not descriptions:\n",
    "            return {\"cov_min\": np.nan, \"cov_mean\": np.nan, \"cov_max\": np.nan}\n",
    "\n",
    "        corpus = descriptions + [str(text)]\n",
    "        vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000).fit_transform(corpus)\n",
    "        desc_vecs = vec[:-1]\n",
    "        text_vec = vec[-1]\n",
    "        sims = cosine_similarity(desc_vecs, text_vec).reshape(-1)\n",
    "\n",
    "        return {\"cov_min\": float(np.min(sims)), \"cov_mean\": float(np.mean(sims)), \"cov_max\": float(np.max(sims))}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def activation_stats_for_feature(texts: list[str], layer: int, unit: int):\n",
    "        \"\"\"\n",
    "        Returns per-text stats:\n",
    "          act_mean = mean over positions of SAE feature activation\n",
    "          act_max  = max over positions of SAE feature activation\n",
    "        \"\"\"\n",
    "        sae = get_sae_for_layer(layer)\n",
    "        hook_name = hook_name_for_layer(layer)\n",
    "\n",
    "        tokens = eval_model.to_tokens(texts).to(device)\n",
    "        _, cache = eval_model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "        acts = cache[hook_name]  # [B, S, d_model]\n",
    "\n",
    "        B, S, D = acts.shape\n",
    "        flat = acts.reshape(-1, D)\n",
    "        feats = sae.encode(flat)                 # [B*S, d_sae]\n",
    "        u = feats[:, int(unit)].reshape(B, S)    # [B, S]\n",
    "\n",
    "        act_mean = u.mean(dim=1).cpu().numpy()\n",
    "        act_max  = u.max(dim=1).values.cpu().numpy()\n",
    "        return act_mean, act_max\n",
    "\n",
    "    # ---- thresholds (start permissive; tighten later) ----\n",
    "    MIN_UNIQUE_TOKEN_RATIO = 0.30\n",
    "    MAX_TRIGRAM_REPEAT = 3\n",
    "    MIN_COV_MIN = 0.05\n",
    "    MIN_ACT_RATIO_VS_SINGLE_MEDIAN = 0.80  # mixed should be at least 80% of typical single activation\n",
    "\n",
    "    qc_parts = []\n",
    "\n",
    "    for (layer, unit), g in texts_df.groupby([\"layer\", \"unit\"]):\n",
    "        layer = int(layer)\n",
    "        unit = int(unit)\n",
    "        g = g.copy()\n",
    "\n",
    "        descs = get_descriptions_for_feature(layer, unit)\n",
    "\n",
    "        # text-only metrics\n",
    "        rep = g[\"text\"].apply(repetition_metrics).apply(pd.Series)\n",
    "        cov = g[\"text\"].apply(lambda t: tfidf_coverage_scores(t, descs)).apply(pd.Series)\n",
    "        g = pd.concat([g.reset_index(drop=True), rep.reset_index(drop=True), cov.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # activation metrics (batched per feature)\n",
    "        act_mean, act_max = activation_stats_for_feature(g[\"text\"].tolist(), layer=layer, unit=unit)\n",
    "        g[\"act_mean\"] = act_mean\n",
    "        g[\"act_max\"] = act_max\n",
    "\n",
    "        # single baseline activation for this feature\n",
    "        singles = g[g[\"sample_type\"] == \"single\"]\n",
    "        single_median = float(np.median(singles[\"act_mean\"])) if len(singles) else np.nan\n",
    "\n",
    "        # flags\n",
    "        g[\"ok_repetition\"] = (g[\"unique_token_ratio\"] >= MIN_UNIQUE_TOKEN_RATIO) & (g[\"max_ngram_count\"] <= MAX_TRIGRAM_REPEAT)\n",
    "        g[\"ok_coverage\"] = (g[\"cov_min\"].fillna(-1.0) >= MIN_COV_MIN)\n",
    "\n",
    "        # activation flag:\n",
    "        # - singles: just require > 0 (or keep all; your choice)\n",
    "        # - mixed: require not “dead” compared to singles\n",
    "        g[\"ok_activation\"] = True\n",
    "        if not np.isnan(single_median) and single_median > 0:\n",
    "            is_mixed = (g[\"sample_type\"] == \"mixed\")\n",
    "            g.loc[is_mixed, \"ok_activation\"] = g.loc[is_mixed, \"act_mean\"] >= (MIN_ACT_RATIO_VS_SINGLE_MEDIAN * single_median)\n",
    "\n",
    "        # final keep decision\n",
    "        g[\"keep\"] = g[\"ok_repetition\"] & g[\"ok_coverage\"] & g[\"ok_activation\"]\n",
    "\n",
    "        qc_parts.append(g)\n",
    "\n",
    "    qc_df = pd.concat(qc_parts, ignore_index=True)\n",
    "\n",
    "    print(\"QC keep rate overall:\", qc_df[\"keep\"].mean())\n",
    "    print(\"QC keep rate by sample_type:\")\n",
    "    print(qc_df.groupby(\"sample_type\")[\"keep\"].mean())\n",
    "\n",
    "    # Save QC table (useful for later debugging/analysis)\n",
    "    qc_path = os.path.join(OUTPUT_DIR, \"prompt_qc_table.csv\")\n",
    "    qc_df.to_csv(qc_path, index=False)\n",
    "    print(\"Saved QC table:\", qc_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
