{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c891e54",
      "metadata": {
        "id": "7c891e54",
        "outputId": "ac65dba3-addb-4070-91db-f64a02a203ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: CONFIG & IMPORTS\n",
        "\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE\n",
        "\n",
        "# ----------------- PATHS: EDIT THESE -----------------\n",
        "\n",
        "BASE_PRISM_DIR = r\"C:\\Users\\thors\\Documents\\GitHub\\prism\"\n",
        "\n",
        "# PRISM descriptions for GPT-2 small SAE\n",
        "DESCRIPTION_DIR = os.path.join(\n",
        "    BASE_PRISM_DIR,\n",
        "    r\"descriptions\\gemini-1-5-pro\\gpt2-small-sae\"\n",
        ")\n",
        "\n",
        "# PRISM polysemanticity + COSY metrics for GPT-2 small SAE\n",
        "METRICS_CSV = os.path.join(\n",
        "    BASE_PRISM_DIR,\n",
        "    r\"results\\meta-evaluation_cosine-similarity_target-gpt2-small-sae_textgen-gemini-1-5-pro_mean_evalgen-gemini-1-5-pro_cosmopedia_1000.csv\"\n",
        ")\n",
        "\n",
        "# Where to save experiment results\n",
        "OUTPUT_DIR = os.path.join(BASE_PRISM_DIR, \"runtime_collision_results\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------- MODELS -----------------\n",
        "\n",
        "# Evaluation model (HookedTransformer) – GPT-2 small\n",
        "EVAL_MODEL_NAME = \"gpt2-small\"  # transformer_lens name\n",
        "\n",
        "# Text generator for constructing A/B/AB prompts.\n",
        "# Can be the same family or something stronger. Start simple.\n",
        "GEN_MODEL_NAME = \"gpt2\"  # HF name; you can swap to a better generator later\n",
        "\n",
        "# SAE release used by PRISM for GPT-2 small: v5, width 32k\n",
        "SAE_RELEASE = \"callummcdougall/sae-gpt2-small-32k-v5\"\n",
        "\n",
        "# ----------------- EXPERIMENT KNOBS -----------------\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# How many different SAE features to analyse in this run\n",
        "N_FEATURES_TO_TEST = 2\n",
        "\n",
        "# How many descriptions (concepts) per feature to use at most\n",
        "MAX_DESCRIPTIONS_PER_FEATURE = 3  # use all if small; cap if large\n",
        "\n",
        "# How many samples per single concept (A, B, C, ...)\n",
        "N_SAMPLES_PER_CONCEPT = 3\n",
        "\n",
        "# How many samples per pair of concepts (AB, AC, ...)\n",
        "N_SAMPLES_PER_PAIR = 3\n",
        "\n",
        "# Max generation length for synthetic prompts\n",
        "MAX_NEW_TOKENS = 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c391b2b7",
      "metadata": {
        "id": "c391b2b7",
        "outputId": "8a556b90-84f1-44fc-a295-266c729c1fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics columns: ['layer', 'unit', 'cosine_similarity', 'cosine_similarity_random', 'max_auc', 'max_mad']\n",
            "Number of feature rows in metrics: 59\n",
            "Found 59 description files\n",
            "Descriptions columns: ['layer', 'unit', 'description', 'mean_activation', 'highlights']\n",
            "Number of description rows: 295\n",
            "Merged columns: ['layer', 'unit', 'description', 'mean_activation', 'highlights', 'cosine_similarity', 'cosine_similarity_random', 'max_auc', 'max_mad']\n",
            "Example merged rows:\n",
            "   layer   unit                                        description  \\\n",
            "0      0  10233  Text editing, looking for something, or solici...   \n",
            "1      0  10233  Requesting, searching for, or providing help, ...   \n",
            "2      0  10233  Requesting, searching for, or looking for some...   \n",
            "3      0  10233             Tools/software, specifically an editor   \n",
            "4      0  10233  Looking for, searching, or requesting somethin...   \n",
            "\n",
            "   mean_activation                                         highlights  \\\n",
            "0         6.202842  ['Text #1:  editor (11.211721420288086)', 'Tex...   \n",
            "1         6.406733  ['Text #1:  sought (6.059510231018066)', 'Text...   \n",
            "2         5.192441  ['Text #1:  seek (7.095390796661377)', 'Text #...   \n",
            "3        10.602112           ['Text #1:  editor (10.60211181640625)']   \n",
            "4         5.651277  ['Text #1:  seek (6.738341808319092)', 'Text #...   \n",
            "\n",
            "   cosine_similarity  cosine_similarity_random  max_auc  max_mad  \n",
            "0           0.568858                  0.353151      0.5      0.0  \n",
            "1           0.568858                  0.353151      0.5      0.0  \n",
            "2           0.568858                  0.353151      0.5      0.0  \n",
            "3           0.568858                  0.353151      0.5      0.0  \n",
            "4           0.568858                  0.353151      0.5      0.0  \n",
            "\n",
            "Top features by number of descriptions:\n",
            "layer  unit \n",
            "0      259      5\n",
            "10     8624     5\n",
            "5      26939    5\n",
            "       28227    5\n",
            "       28452    5\n",
            "       28636    5\n",
            "       30319    5\n",
            "       31286    5\n",
            "       32177    5\n",
            "10     900      5\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: LOAD PRISM DESCRIPTIONS + METRICS FOR GPT-2 SMALL SAE\n",
        "\n",
        "import glob\n",
        "\n",
        "# 1) Load polysemanticity + description quality metrics\n",
        "metrics_df = pd.read_csv(METRICS_CSV)\n",
        "print(\"Metrics columns:\", metrics_df.columns.tolist())\n",
        "print(\"Number of feature rows in metrics:\", len(metrics_df))\n",
        "\n",
        "# 2) Load all description CSVs for gpt2-small-sae\n",
        "desc_files = glob.glob(os.path.join(DESCRIPTION_DIR, \"gpt2-small-sae_layer-*_unit-*.csv\"))\n",
        "print(f\"Found {len(desc_files)} description files\")\n",
        "\n",
        "desc_dfs = []\n",
        "for path in desc_files:\n",
        "    df = pd.read_csv(path)\n",
        "    # Expect columns: layer,unit,description,mean_activation,highlights (your example)\n",
        "    desc_dfs.append(df)\n",
        "\n",
        "descriptions_df = pd.concat(desc_dfs, ignore_index=True)\n",
        "print(\"Descriptions columns:\", descriptions_df.columns.tolist())\n",
        "print(\"Number of description rows:\", len(descriptions_df))\n",
        "\n",
        "# 3) Merge descriptions with metrics on (layer, unit)\n",
        "merged_df = descriptions_df.merge(metrics_df, on=[\"layer\", \"unit\"], how=\"left\")\n",
        "\n",
        "print(\"Merged columns:\", merged_df.columns.tolist())\n",
        "print(\"Example merged rows:\")\n",
        "print(merged_df.head(5))\n",
        "\n",
        "# Utility: list all features with at least 2 descriptions\n",
        "feature_counts = merged_df.groupby([\"layer\", \"unit\"]).size().sort_values(ascending=False)\n",
        "print(\"\\nTop features by number of descriptions:\")\n",
        "print(feature_counts.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32fd266",
      "metadata": {
        "id": "b32fd266",
        "outputId": "07a11475-8f09-4d14-d47f-6f49df03f2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 59 features with >= 2 descriptions.\n",
            "First few candidate features: [(0, 259), (0, 2002), (0, 2236), (0, 2332), (0, 3358), (0, 5369), (0, 8966), (0, 9661), (0, 10233), (0, 10917)]\n",
            "\n",
            "Will test these features (layer, unit): [(0, 259), (0, 2002)]\n",
            "\n",
            "Feature (layer=0, unit=259) with 3 descriptions:\n",
            "  concept[0] description: Diseases and negative attributes\n",
            "  concept[1] description: Diseases, medical conditions, or health issues\n",
            "  concept[2] description: Medical studies of the effects of various factors or substances on different types of cancer\n",
            "  cosine_similarity: 0.4671536862850189\n",
            "  max_auc: 0.5 max_mad: 0.0\n",
            "\n",
            "Feature (layer=0, unit=2002) with 3 descriptions:\n",
            "  concept[0] description: Data storage, transfer, or management, often in relation to websites, software, or online platforms\n",
            "  concept[1] description: Homemade items, products, food, or creative works\n",
            "  concept[2] description: Male names in the context of job titles/roles\n",
            "  cosine_similarity: 0.2893633708357811\n",
            "  max_auc: 0.54305 max_mad: 0.2536145883724486\n"
          ]
        }
      ],
      "source": [
        "# CELL 3: SELECT FEATURES TO TEST AND PREP CONCEPT LISTS\n",
        "\n",
        "# Group by (layer, unit)\n",
        "grouped = merged_df.groupby([\"layer\", \"unit\"])\n",
        "\n",
        "# Get candidate features with at least 2 descriptions\n",
        "candidate_features = []\n",
        "for (layer, unit), g in grouped:\n",
        "    if len(g) >= 2:\n",
        "        candidate_features.append((layer, unit))\n",
        "\n",
        "print(f\"Found {len(candidate_features)} features with >= 2 descriptions.\")\n",
        "print(\"First few candidate features:\", candidate_features[:10])\n",
        "\n",
        "# Take the first N_FEATURES_TO_TEST for this run\n",
        "features_to_test = candidate_features[:N_FEATURES_TO_TEST]\n",
        "print(f\"\\nWill test these features (layer, unit): {features_to_test}\")\n",
        "\n",
        "# Build a dict: (layer, unit) -> description rows + metrics\n",
        "feature_concepts = {}\n",
        "\n",
        "for (layer, unit) in features_to_test:\n",
        "    g = grouped.get_group((layer, unit)).reset_index(drop=True)\n",
        "    # Optionally subsample descriptions if there are many\n",
        "    if len(g) > MAX_DESCRIPTIONS_PER_FEATURE:\n",
        "        g = g.sample(n=MAX_DESCRIPTIONS_PER_FEATURE, random_state=SEED).reset_index(drop=True)\n",
        "    feature_concepts[(layer, unit)] = g\n",
        "\n",
        "# Quick inspection\n",
        "for (layer, unit), df_feat in feature_concepts.items():\n",
        "    print(f\"\\nFeature (layer={layer}, unit={unit}) with {len(df_feat)} descriptions:\")\n",
        "    for i, row in df_feat.iterrows():\n",
        "        print(f\"  concept[{i}] description: {row['description']}\")\n",
        "    print(\"  cosine_similarity:\", df_feat['cosine_similarity'].iloc[0])\n",
        "    print(\"  max_auc:\", df_feat['max_auc'].iloc[0], \"max_mad:\", df_feat['max_mad'].iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca5a839",
      "metadata": {
        "id": "5ca5a839",
        "outputId": "8b77211a-8494-4d2c-e3bb-44c89a319cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n",
            "Loaded eval model: gpt2-small\n",
            "Max PRISM unit index per layer: {0: 32439, 5: 32177, 10: 27300}\n"
          ]
        }
      ],
      "source": [
        "# CELL 4 (UPDATED): LOAD GPT-2 SMALL + SAE PER LAYER, MATCHING PRISM\n",
        "\n",
        "from sae_lens import SAE\n",
        "\n",
        "# Load eval model once\n",
        "eval_model = HookedTransformer.from_pretrained(\n",
        "    EVAL_MODEL_NAME,\n",
        "    device=device,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "eval_model.eval()\n",
        "print(\"Loaded eval model:\", EVAL_MODEL_NAME)\n",
        "\n",
        "# ---- SAE RELEASE PRISM USES FOR GPT-2 SMALL SAE v5 (32k, resid_post) ----\n",
        "SAE_RELEASE = \"gpt2-small-resid-post-v5-32k\"\n",
        "\n",
        "# Pre-compute, from merged_df, the max unit index per layer that PRISM uses\n",
        "max_unit_by_layer = merged_df.groupby(\"layer\")[\"unit\"].max().to_dict()\n",
        "print(\"Max PRISM unit index per layer:\", max_unit_by_layer)\n",
        "\n",
        "sae_cache = {}\n",
        "\n",
        "def get_sae_for_layer(layer: int) -> SAE:\n",
        "    \"\"\"\n",
        "    Load (and cache) the SAE for a given layer.\n",
        "\n",
        "    Assumes the SAE in SAE_RELEASE is stored under\n",
        "        sae_id = f\"blocks.{layer}.hook_resid_post\"\n",
        "    which is the standard for GPT-2-small resid_post SAEs.\n",
        "\n",
        "    Also sanity-checks that the SAE has enough features to cover\n",
        "    the max PRISM 'unit' index for this layer.\n",
        "    \"\"\"\n",
        "    if layer in sae_cache:\n",
        "        return sae_cache[layer]\n",
        "\n",
        "    sae_id = f\"blocks.{layer}.hook_resid_post\"\n",
        "    print(f\"\\nLoading SAE for layer {layer}: release={SAE_RELEASE}, sae_id={sae_id}\")\n",
        "\n",
        "    sae, cfg, sparsity = SAE.from_pretrained_with_cfg_and_sparsity(\n",
        "        SAE_RELEASE,\n",
        "        sae_id,\n",
        "        device=device,\n",
        "    )\n",
        "    sae.eval()\n",
        "\n",
        "    # Sanity check: SAE feature width must exceed max PRISM unit for this layer\n",
        "    if layer in max_unit_by_layer:\n",
        "        max_unit = max_unit_by_layer[layer]\n",
        "\n",
        "        W0, W1 = sae.W_dec.shape  # two dims\n",
        "        d_model = eval_model.cfg.d_model  # 768 for gpt2-small\n",
        "\n",
        "        # Decide which axis is features and which is d_model\n",
        "        if W0 == d_model and W1 != d_model:\n",
        "            n_features = W1\n",
        "        elif W1 == d_model and W0 != d_model:\n",
        "            n_features = W0\n",
        "        else:\n",
        "            # Fallback: assume features is the larger dimension\n",
        "            n_features = max(W0, W1)\n",
        "            print(\n",
        "                f\"Warning: couldn't infer orientation of W_dec cleanly; \"\n",
        "                f\"using n_features = max({W0}, {W1}) = {n_features}\"\n",
        "            )\n",
        "\n",
        "        if max_unit >= n_features:\n",
        "            raise ValueError(\n",
        "                f\"PRISM units go up to {max_unit} in layer {layer}, \"\n",
        "                f\"but SAE '{SAE_RELEASE}/{sae_id}' only has {n_features} features \"\n",
        "                f\"(W_dec.shape={sae.W_dec.shape}). \"\n",
        "                \"This means you're using the wrong SAE release for these descriptions.\"\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Layer {layer}: SAE W_dec.shape={sae.W_dec.shape}, \"\n",
        "                f\"inferred n_features={n_features}, max PRISM unit={max_unit} -> OK.\"\n",
        "            )\n",
        "\n",
        "    sae_cache[layer] = sae\n",
        "    return sae\n",
        "\n",
        "\n",
        "def hook_name_for_layer(layer: int) -> str:\n",
        "    \"\"\"Return the TransformerLens hook name for this layer's resid_post.\"\"\"\n",
        "    return f\"blocks.{layer}.hook_resid_post\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673d6aca",
      "metadata": {
        "id": "673d6aca"
      },
      "outputs": [],
      "source": [
        "# CELL 5: LOAD TEXT GENERATOR MODEL + HELPER\n",
        "\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(GEN_MODEL_NAME).to(device)\n",
        "gen_model.eval()\n",
        "\n",
        "if gen_tokenizer.pad_token_id is None:\n",
        "    gen_tokenizer.pad_token_id = gen_tokenizer.eos_token_id\n",
        "\n",
        "def generate_samples(prompt: str, n_samples: int, max_new_tokens: int = MAX_NEW_TOKENS):\n",
        "    \"\"\"Generate n_samples texts from the generator model given a prompt.\"\"\"\n",
        "    samples = []\n",
        "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    for _ in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            out = gen_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.8,\n",
        "                top_p=0.95,\n",
        "                pad_token_id=gen_tokenizer.pad_token_id,\n",
        "            )\n",
        "        full_text = gen_tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "        if full_text.startswith(prompt):\n",
        "            continuation = full_text[len(prompt):].strip()\n",
        "        else:\n",
        "            continuation = full_text\n",
        "        samples.append(continuation)\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ac27ee",
      "metadata": {
        "id": "27ac27ee"
      },
      "outputs": [],
      "source": [
        "# CELL 6: LOSS COMPUTATION HELPERS (BASELINE + INTERVENTION)\n",
        "\n",
        "def compute_losses_baseline(texts):\n",
        "    \"\"\"Baseline per-sample loss with no intervention.\"\"\"\n",
        "    if not texts:\n",
        "        return np.array([]), float(\"nan\")\n",
        "\n",
        "    tokens = eval_model.to_tokens(texts).to(device)\n",
        "    input_ids = tokens\n",
        "    labels = tokens.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = eval_model(input_ids, return_type=\"logits\")\n",
        "\n",
        "    shift_logits = logits[:, :-1, :]\n",
        "    shift_labels = labels[:, 1:]\n",
        "\n",
        "    logp = torch.log_softmax(shift_logits, dim=-1)\n",
        "    nll = -logp.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    token_counts = torch.ones_like(shift_labels, dtype=torch.float32).sum(dim=1)\n",
        "    per_sample_loss = nll.sum(dim=1) / token_counts\n",
        "\n",
        "    return per_sample_loss.cpu().numpy(), float(per_sample_loss.mean().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315260b7",
      "metadata": {
        "id": "315260b7"
      },
      "outputs": [],
      "source": [
        "# CELL 7 (UPDATED): SAE INTERVENTION HOOK (PER-LAYER) + LOSS WITH INTERVENTION\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def sae_intervention_hook(\n",
        "    acts: torch.Tensor,\n",
        "    hook,\n",
        "    sae: SAE,\n",
        "    feature_indices,\n",
        "    mode: str = \"ablate\",\n",
        "    clamp_values=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    acts: [batch, seq, d_model] activation at blocks.{layer}.hook_resid_post\n",
        "    mode:\n",
        "      - \"ablate\": set selected features to 0\n",
        "      - \"clamp\": set selected features to clamp_values (same length as feature_indices)\n",
        "    \"\"\"\n",
        "    assert mode in (\"ablate\", \"clamp\")\n",
        "\n",
        "    bsz, seq_len, d_model = acts.shape\n",
        "    acts_flat = acts.reshape(-1, d_model)  # [B*T, d_model]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = sae.encode(acts_flat)  # [B*T, n_features]\n",
        "\n",
        "        if not isinstance(feature_indices, (list, tuple, np.ndarray)):\n",
        "            feature_indices_list = [feature_indices]\n",
        "        else:\n",
        "            feature_indices_list = list(feature_indices)\n",
        "\n",
        "        if mode == \"ablate\":\n",
        "            feats[:, feature_indices_list] = 0.0\n",
        "        elif mode == \"clamp\":\n",
        "            assert clamp_values is not None\n",
        "            assert len(clamp_values) == len(feature_indices_list)\n",
        "            for idx, val in zip(feature_indices_list, clamp_values):\n",
        "                feats[:, idx] = val\n",
        "\n",
        "        W_dec = sae.W_dec.to(acts_flat.dtype).to(acts_flat.device)\n",
        "        b_dec = sae.b_dec.to(acts_flat.dtype).to(acts_flat.device)  # [d_model]\n",
        "\n",
        "        d_model = acts_flat.shape[-1]\n",
        "        W0, W1 = W_dec.shape\n",
        "\n",
        "        # Case 1: W_dec is [d_model, n_features]\n",
        "        if W0 == d_model and W1 != d_model:\n",
        "            # feats: [B*T, n_features], W_dec.T: [n_features, d_model]\n",
        "            recon_flat = feats @ W_dec.T + b_dec  # [B*T, d_model]\n",
        "\n",
        "        # Case 2: W_dec is [n_features, d_model]\n",
        "        elif W1 == d_model and W0 != d_model:\n",
        "            # feats: [B*T, n_features], W_dec: [n_features, d_model]\n",
        "            recon_flat = feats @ W_dec + b_dec    # [B*T, d_model]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unexpected W_dec shape {W_dec.shape} for d_model={d_model}. \"\n",
        "                \"Can't infer how to decode features.\"\n",
        "            )\n",
        "\n",
        "    recon = recon_flat.reshape(bsz, seq_len, d_model)\n",
        "    return recon\n",
        "\n",
        "\n",
        "def compute_losses_with_intervention(\n",
        "    texts,\n",
        "    sae: SAE,\n",
        "    hook_name: str,\n",
        "    feature_indices,\n",
        "    mode: str = \"ablate\",\n",
        "    clamp_values=None,\n",
        "):\n",
        "    \"\"\"Compute per-sample loss with SAE intervention at a specific layer.\"\"\"\n",
        "    if not texts:\n",
        "        return np.array([]), float(\"nan\")\n",
        "\n",
        "    tokens = eval_model.to_tokens(texts).to(device)\n",
        "    input_ids = tokens\n",
        "    labels = tokens.clone()\n",
        "\n",
        "    hook_fn = partial(\n",
        "        sae_intervention_hook,\n",
        "        sae=sae,\n",
        "        feature_indices=feature_indices,\n",
        "        mode=mode,\n",
        "        clamp_values=clamp_values,\n",
        "    )\n",
        "    fwd_hooks = [(hook_name, hook_fn)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = eval_model.run_with_hooks(\n",
        "            input_ids,\n",
        "            return_type=\"logits\",\n",
        "            fwd_hooks=fwd_hooks,\n",
        "        )\n",
        "\n",
        "    shift_logits = logits[:, :-1, :]\n",
        "    shift_labels = labels[:, 1:]\n",
        "\n",
        "    logp = torch.log_softmax(shift_logits, dim=-1)\n",
        "    nll = -logp.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    token_counts = torch.ones_like(shift_labels, dtype=torch.float32).sum(dim=1)\n",
        "    per_sample_loss = nll.sum(dim=1) / token_counts\n",
        "\n",
        "    return per_sample_loss.cpu().numpy(), float(per_sample_loss.mean().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc9b95b",
      "metadata": {
        "id": "0fc9b95b",
        "outputId": "653ba806-510c-4cc8-af4e-ae91c5ddd964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Generating data for feature (layer=0, unit=259) ===\n",
            "Descriptions:\n",
            "  concept[0]: Diseases and negative attributes\n",
            "  concept[1]: Diseases, medical conditions, or health issues\n",
            "  concept[2]: Medical studies of the effects of various factors or substances on different types of cancer\n",
            "\n",
            "=== Generating data for feature (layer=0, unit=2002) ===\n",
            "Descriptions:\n",
            "  concept[0]: Data storage, transfer, or management, often in relation to websites, software, or online platforms\n",
            "  concept[1]: Homemade items, products, food, or creative works\n",
            "  concept[2]: Male names in the context of job titles/roles\n",
            "\n",
            "Finished generating prompts for all selected features.\n"
          ]
        }
      ],
      "source": [
        "# CELL 8: PROMPT TEMPLATES + GENERATION\n",
        "\n",
        "def build_single_concept_prompt(desc: str) -> str:\n",
        "    return (\n",
        "        \"Write a short paragraph (4–6 sentences) that strongly involves the following theme:\\n\"\n",
        "        f\"\\\"{desc}\\\"\\n\"\n",
        "        \"Focus ONLY on this theme.\\n\"\n",
        "        \"Avoid mentioning or alluding to unrelated topics.\\n\\n\"\n",
        "    )\n",
        "\n",
        "def build_pair_concept_prompt(desc1: str, desc2: str) -> str:\n",
        "    return (\n",
        "        \"Write a short paragraph (6–8 sentences) that strongly involves BOTH of the following themes:\\n\"\n",
        "        f\"1. \\\"{desc1}\\\"\\n\"\n",
        "        f\"2. \\\"{desc2}\\\"\\n\\n\"\n",
        "        \"Make sure both themes appear multiple times and interact in a coherent way.\\n\"\n",
        "        \"Do not write a list; write a natural paragraph.\\n\\n\"\n",
        "    )\n",
        "\n",
        "all_feature_entries = []\n",
        "\n",
        "for (layer, unit), df_feat in feature_concepts.items():\n",
        "    print(f\"\\n=== Generating data for feature (layer={layer}, unit={unit}) ===\")\n",
        "    descriptions = df_feat[\"description\"].tolist()\n",
        "    n_desc = len(descriptions)\n",
        "    print(\"Descriptions:\")\n",
        "    for i, d in enumerate(descriptions):\n",
        "        print(f\"  concept[{i}]: {d}\")\n",
        "\n",
        "    single_texts = {}  # concept index -> list of texts\n",
        "    pair_texts = {}    # (i, j) -> list of texts\n",
        "\n",
        "    # Single-concept texts\n",
        "    for i, desc in enumerate(descriptions):\n",
        "        prompt = build_single_concept_prompt(desc)\n",
        "        texts_i = generate_samples(prompt, n_samples=N_SAMPLES_PER_CONCEPT)\n",
        "        single_texts[i] = texts_i\n",
        "\n",
        "    # Pair-concept texts (all unordered pairs)\n",
        "    for i, j in itertools.combinations(range(n_desc), 2):\n",
        "        desc_i, desc_j = descriptions[i], descriptions[j]\n",
        "        prompt_pair = build_pair_concept_prompt(desc_i, desc_j)\n",
        "        texts_ij = generate_samples(prompt_pair, n_samples=N_SAMPLES_PER_PAIR)\n",
        "        pair_texts[(i, j)] = texts_ij\n",
        "\n",
        "    all_feature_entries.append(\n",
        "        {\n",
        "            \"layer\": layer,\n",
        "            \"unit\": unit,\n",
        "            \"df_feat\": df_feat,\n",
        "            \"descriptions\": descriptions,\n",
        "            \"single_texts\": single_texts,\n",
        "            \"pair_texts\": pair_texts,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\\nFinished generating prompts for all selected features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae44bd0",
      "metadata": {
        "id": "fae44bd0",
        "outputId": "703fac00-8180-41b1-c881-34aa1cd4a888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading SAE for layer 0: release=gpt2-small-resid-post-v5-32k, sae_id=blocks.0.hook_resid_post\n",
            "Layer 0: SAE W_dec.shape=torch.Size([32768, 768]), inferred n_features=32768, max PRISM unit=32439 -> OK.\n",
            "\n",
            "=== Evaluating feature (layer=0, unit=259) ===\n",
            "\n",
            "=== Evaluating feature (layer=0, unit=2002) ===\n",
            "\n",
            "Finished running losses for all features and prompts.\n"
          ]
        }
      ],
      "source": [
        "# CELL 9: RUN BASELINE + ABLATION LOSSES FOR EACH FEATURE\n",
        "\n",
        "results_rows = []\n",
        "\n",
        "for feat_entry in all_feature_entries:\n",
        "    layer = feat_entry[\"layer\"]\n",
        "    unit = feat_entry[\"unit\"]\n",
        "    df_feat = feat_entry[\"df_feat\"]\n",
        "    descriptions = feat_entry[\"descriptions\"]\n",
        "    single_texts = feat_entry[\"single_texts\"]\n",
        "    pair_texts = feat_entry[\"pair_texts\"]\n",
        "\n",
        "    # Load correct SAE and hook_name for this layer\n",
        "    sae = get_sae_for_layer(layer)\n",
        "    hook_name = hook_name_for_layer(layer)\n",
        "\n",
        "    feature_idx = unit  # PRISM's 'unit' is the SAE feature index for this layer\n",
        "\n",
        "    print(f\"\\n=== Evaluating feature (layer={layer}, unit={unit}) ===\")\n",
        "\n",
        "    # SINGLE-CONCEPT PROMPTS\n",
        "    for concept_id, texts in single_texts.items():\n",
        "        concept_label = f\"C{concept_id}\"\n",
        "\n",
        "        # Baseline\n",
        "        losses_base, _ = compute_losses_baseline(texts)\n",
        "\n",
        "        # Ablated (feature_idx)\n",
        "        losses_abl, _ = compute_losses_with_intervention(\n",
        "            texts,\n",
        "            sae=sae,\n",
        "            hook_name=hook_name,\n",
        "            feature_indices=[feature_idx],\n",
        "            mode=\"ablate\",\n",
        "            clamp_values=None,\n",
        "        )\n",
        "\n",
        "        for i, (text, lb, la) in enumerate(zip(texts, losses_base, losses_abl)):\n",
        "            results_rows.append(\n",
        "                {\n",
        "                    \"layer\": layer,\n",
        "                    \"unit\": unit,\n",
        "                    \"concept_set\": concept_label,\n",
        "                    \"sample_type\": \"single\",\n",
        "                    \"sample_idx\": i,\n",
        "                    \"mode\": \"baseline\",\n",
        "                    \"loss\": float(lb),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "            results_rows.append(\n",
        "                {\n",
        "                    \"layer\": layer,\n",
        "                    \"unit\": unit,\n",
        "                    \"concept_set\": concept_label,\n",
        "                    \"sample_type\": \"single\",\n",
        "                    \"sample_idx\": i,\n",
        "                    \"mode\": \"ablate\",\n",
        "                    \"loss\": float(la),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # PAIR-CONCEPT PROMPTS\n",
        "    for (i, j), texts in pair_texts.items():\n",
        "        concept_label = f\"C{i}+C{j}\"\n",
        "\n",
        "        losses_base, _ = compute_losses_baseline(texts)\n",
        "        losses_abl, _ = compute_losses_with_intervention(\n",
        "            texts,\n",
        "            sae=sae,\n",
        "            hook_name=hook_name,\n",
        "            feature_indices=[feature_idx],\n",
        "            mode=\"ablate\",\n",
        "            clamp_values=None,\n",
        "        )\n",
        "\n",
        "        for k, (text, lb, la) in enumerate(zip(texts, losses_base, losses_abl)):\n",
        "            results_rows.append(\n",
        "                {\n",
        "                    \"layer\": layer,\n",
        "                    \"unit\": unit,\n",
        "                    \"concept_set\": concept_label,\n",
        "                    \"sample_type\": \"pair\",\n",
        "                    \"sample_idx\": k,\n",
        "                    \"mode\": \"baseline\",\n",
        "                    \"loss\": float(lb),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "            results_rows.append(\n",
        "                {\n",
        "                    \"layer\": layer,\n",
        "                    \"unit\": unit,\n",
        "                    \"concept_set\": concept_label,\n",
        "                    \"sample_type\": \"pair\",\n",
        "                    \"sample_idx\": k,\n",
        "                    \"mode\": \"ablate\",\n",
        "                    \"loss\": float(la),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "print(\"\\nFinished running losses for all features and prompts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a10f5825",
      "metadata": {
        "id": "a10f5825",
        "outputId": "ec7cdb30-2414-4eaa-81c3-546db0d88c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results head:\n",
            "   layer  unit concept_set sample_type  sample_idx      mode      loss  \\\n",
            "0      0   259          C0      single           0  baseline  1.795703   \n",
            "1      0   259          C0      single           0    ablate  1.994194   \n",
            "2      0   259          C0      single           1  baseline  2.275754   \n",
            "3      0   259          C0      single           1    ablate  2.667865   \n",
            "4      0   259          C0      single           2  baseline  2.172371   \n",
            "\n",
            "                                                text  \n",
            "0  If you don't have a strong desire to read, don...  \n",
            "1  If you don't have a strong desire to read, don...  \n",
            "2  What the title should not be.\\n\\n\\nYou can use...  \n",
            "3  What the title should not be.\\n\\n\\nYou can use...  \n",
            "4  \"The role of the eye\"\\n\\nThe role of the eye i...  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'out_path = os.path.join(OUTPUT_DIR, \"runtime_collision_gpt2_small_sae_raw.csv\")\\nresults_df.to_csv(out_path, index=False)\\nprint(f\"\\nSaved raw results to: {out_path}\")'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# CELL 10: ASSEMBLE RESULTS AND SAVE\n",
        "\n",
        "results_df = pd.DataFrame(results_rows)\n",
        "print(\"Results head:\")\n",
        "print(results_df.head())\n",
        "\n",
        "\"\"\"out_path = os.path.join(OUTPUT_DIR, \"runtime_collision_gpt2_small_sae_raw.csv\")\n",
        "results_df.to_csv(out_path, index=False)\n",
        "print(f\"\\nSaved raw results to: {out_path}\")\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55d49be",
      "metadata": {
        "id": "d55d49be",
        "outputId": "d6d74e0c-47d4-439a-9dab-2ae129c06434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pivot columns: ['layer', 'unit', 'concept_set', 'sample_type', 'sample_idx', 'ablate', 'baseline']\n",
            "Example per-sample deltas:\n",
            "mode  layer  unit concept_set sample_type  sample_idx    ablate  baseline  \\\n",
            "0         0   259          C0      single           0  1.994194  1.795703   \n",
            "1         0   259          C0      single           1  2.667865  2.275754   \n",
            "2         0   259          C0      single           2  2.351218  2.172371   \n",
            "3         0   259       C0+C1        pair           0  2.203298  1.972865   \n",
            "4         0   259       C0+C1        pair           1  1.294833  1.242488   \n",
            "\n",
            "mode     delta  \n",
            "0     0.198491  \n",
            "1     0.392111  \n",
            "2     0.178847  \n",
            "3     0.230433  \n",
            "4     0.052345  \n"
          ]
        }
      ],
      "source": [
        "# CELL B: COMPUTE PER-SAMPLE DELTA LOSS (ABLATE - BASELINE)\n",
        "\n",
        "# Pivot modes so we can compute delta per sample\n",
        "pivot_cols = [\"layer\", \"unit\", \"concept_set\", \"sample_type\", \"sample_idx\"]\n",
        "pivot_df = results_df.pivot_table(\n",
        "    index=pivot_cols,\n",
        "    columns=\"mode\",\n",
        "    values=\"loss\"\n",
        ").reset_index()\n",
        "\n",
        "# sanity check columns\n",
        "print(\"Pivot columns:\", pivot_df.columns.tolist())\n",
        "\n",
        "# Compute delta = ablate - baseline\n",
        "pivot_df[\"delta\"] = pivot_df[\"ablate\"] - pivot_df[\"baseline\"]\n",
        "\n",
        "print(\"Example per-sample deltas:\")\n",
        "print(pivot_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8e1ff3",
      "metadata": {
        "id": "7b8e1ff3",
        "outputId": "9ec54c4c-fccb-4946-dab4-4c7c98324591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per (feature, concept_set) summary:\n",
            "   layer  unit sample_type concept_set  mean_baseline  mean_ablate  \\\n",
            "0      0   259        pair       C0+C1       1.556598     1.694588   \n",
            "1      0   259        pair       C0+C2       1.798201     2.035471   \n",
            "2      0   259        pair       C1+C2       2.029880     2.235707   \n",
            "3      0   259      single          C0       2.081276     2.337759   \n",
            "4      0   259      single          C1       2.061047     2.293462   \n",
            "\n",
            "   mean_delta  n_samples  \n",
            "0    0.137990          3  \n",
            "1    0.237270          3  \n",
            "2    0.205827          3  \n",
            "3    0.256483          3  \n",
            "4    0.232415          3  \n"
          ]
        }
      ],
      "source": [
        "# CELL C: PER-FEATURE SUMMARY (SINGLE vs PAIR)\n",
        "\n",
        "group_cols = [\"layer\", \"unit\", \"sample_type\", \"concept_set\"]\n",
        "\n",
        "agg_df = pivot_df.groupby(group_cols).agg(\n",
        "    mean_baseline=(\"baseline\", \"mean\"),\n",
        "    mean_ablate=(\"ablate\", \"mean\"),\n",
        "    mean_delta=(\"delta\", \"mean\"),\n",
        "    n_samples=(\"delta\", \"size\"),\n",
        ").reset_index()\n",
        "\n",
        "print(\"Per (feature, concept_set) summary:\")\n",
        "print(agg_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fcd94c",
      "metadata": {
        "id": "e3fcd94c",
        "outputId": "c6ceb4ae-e4cd-480d-fbfc-f42084eb984d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-feature collision summary (first 10):\n",
            "   layer  unit  delta_single_mean  delta_single_std  n_single_sets  \\\n",
            "0      0   259           0.297919          0.093391              3   \n",
            "1      0  2002           0.413501          0.159108              3   \n",
            "\n",
            "   delta_pair_mean  delta_pair_std  n_pair_sets  collision_penalty  \n",
            "0         0.193696        0.050740            3          -0.104223  \n",
            "1        -0.345204        1.040562            3          -0.758705  \n"
          ]
        }
      ],
      "source": [
        "# CELL D: COLLISION METRIC PER FEATURE\n",
        "\n",
        "# Separate singles and pairs\n",
        "singles = agg_df[agg_df[\"sample_type\"] == \"single\"]\n",
        "pairs   = agg_df[agg_df[\"sample_type\"] == \"pair\"]\n",
        "\n",
        "# mean over concept_sets for each feature\n",
        "single_feat = singles.groupby([\"layer\", \"unit\"]).agg(\n",
        "    delta_single_mean=(\"mean_delta\", \"mean\"),\n",
        "    delta_single_std=(\"mean_delta\", \"std\"),\n",
        "    n_single_sets=(\"mean_delta\", \"size\"),\n",
        ").reset_index()\n",
        "\n",
        "pair_feat = pairs.groupby([\"layer\", \"unit\"]).agg(\n",
        "    delta_pair_mean=(\"mean_delta\", \"mean\"),\n",
        "    delta_pair_std=(\"mean_delta\", \"std\"),\n",
        "    n_pair_sets=(\"mean_delta\", \"size\"),\n",
        ").reset_index()\n",
        "\n",
        "# merge\n",
        "feat_summary = single_feat.merge(\n",
        "    pair_feat,\n",
        "    on=[\"layer\", \"unit\"],\n",
        "    how=\"outer\",\n",
        "    suffixes=(\"_single\", \"_pair\")\n",
        ")\n",
        "\n",
        "# Compute collision penalty where both exist\n",
        "feat_summary[\"collision_penalty\"] = (\n",
        "    feat_summary[\"delta_pair_mean\"] - feat_summary[\"delta_single_mean\"]\n",
        ")\n",
        "\n",
        "print(\"Per-feature collision summary (first 10):\")\n",
        "print(feat_summary.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1448ca",
      "metadata": {
        "id": "6b1448ca",
        "outputId": "2952547f-34d4-4b8e-f099-46c59cab5cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature-level dataframe with collision + PRISM metrics:\n",
            "   layer  unit  delta_single_mean  delta_single_std  n_single_sets  \\\n",
            "0      0   259           0.297919          0.093391              3   \n",
            "1      0  2002           0.413501          0.159108              3   \n",
            "\n",
            "   delta_pair_mean  delta_pair_std  n_pair_sets  collision_penalty  \\\n",
            "0         0.193696        0.050740            3          -0.104223   \n",
            "1        -0.345204        1.040562            3          -0.758705   \n",
            "\n",
            "   cosine_similarity  cosine_similarity_random  max_auc   max_mad  \n",
            "0           0.467154                  0.333642  0.50000  0.000000  \n",
            "1           0.289363                  0.281192  0.54305  0.253615  \n",
            "\n",
            "Top 10 features by collision_penalty:\n",
            "   layer  unit  collision_penalty  delta_single_mean  delta_pair_mean  \\\n",
            "0      0   259          -0.104223           0.297919         0.193696   \n",
            "1      0  2002          -0.758705           0.413501        -0.345204   \n",
            "\n",
            "   cosine_similarity  max_auc   max_mad  \n",
            "0           0.467154  0.50000  0.000000  \n",
            "1           0.289363  0.54305  0.253615  \n"
          ]
        }
      ],
      "source": [
        "# CELL E: JOIN WITH PRISM METRICS\n",
        "\n",
        "# Keep unique per-feature metrics from merged_df\n",
        "metrics_cols = [\"layer\", \"unit\", \"cosine_similarity\", \"cosine_similarity_random\", \"max_auc\", \"max_mad\"]\n",
        "metrics_unique = merged_df[metrics_cols].drop_duplicates(subset=[\"layer\", \"unit\"])\n",
        "\n",
        "feat_with_metrics = feat_summary.merge(\n",
        "    metrics_unique,\n",
        "    on=[\"layer\", \"unit\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"Feature-level dataframe with collision + PRISM metrics:\")\n",
        "print(feat_with_metrics.head())\n",
        "\n",
        "# Example: look at top features by collision_penalty\n",
        "print(\"\\nTop 10 features by collision_penalty:\")\n",
        "print(\n",
        "    feat_with_metrics.sort_values(\"collision_penalty\", ascending=False)\n",
        "                     .head(10)[[\"layer\", \"unit\", \"collision_penalty\", \"delta_single_mean\", \"delta_pair_mean\", \"cosine_similarity\", \"max_auc\", \"max_mad\"]]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}